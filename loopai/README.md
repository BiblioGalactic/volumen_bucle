# 🧠 bucleia – Self‑generative AI in Local Loop

This script allows you to launch a local model (based on `llama.cpp`) with **auto‑synthetic** behaviour:
the AI starts from a single initial instruction (`loop_prompt.txt`) and evolves autonomously, rewriting its own input (`nexus.txt`) after each execution.

## ⚙️ What does it do?

* Runs a `.gguf` model using a combined prompt of the system and the last generated input.
* Filters the output to capture only the useful block generated by the AI (between `[[` and `]]`).
* Automatically rewrites the input file (`nexus.txt`) with the generated response.
* Saves a complete copy of each session in `sesiones/` and full logs in `logs/`.

> Each time it runs, the system feeds back on itself: **the AI instructs itself** from its own previous output.

## 📁 Expected structure

In the same directory as the `loopai.sh` script there must be the following files:

* `llama-cli` → binary from `llama.cpp`
* `mistral-7b-instruct-v0.1.Q6_K.gguf` → local model
* `loop_prompt.txt` → system initial prompt
* `nexus.txt` → first input, overwritten with each response

And they will be generated automatically:

* `sesiones/` → .md copies of each generated session
* `logs/` → complete execution logs

## 🧪 Requirements

* Bash (macOS / Linux)
* Compiled `llama.cpp`
* A compatible `.gguf` model

## 🚀 Execution

```bash
chmod +x loopai.sh
./loopai.sh
```

## 💡 Creative use

Ideal for loop simulations, autonomous narrative agents or AI experiments that need to evolve without external intervention.
Designed for offline, reproducible and fully local environments.

---

Created by **Eto Demerzel**

**Eto Demerzel** (Gustavo Silva Da Costa)
https://etodemerzel.gumroad.com  
https://github.com/BiblioGalactic
