# 🧠 bucleia – ローカルループ内の自己生成AI

このスクリプトは、`llama.cpp` ベースの **自己合成的** な動作を持つローカルモデルを起動します。
AI は単一の初期指示（`ループプロンプト.txt`）からスタートし、各実行後に自分の入力（`ネクサス.txt`）を書き換えながら自律的に進化します。

## ⚙️ 何をするのか?

* システムのプロンプトと最後に生成された入力を組み合わせたプロンプトで `.gguf` モデルを実行します。
* 出力をフィルタし、AI が生成した有用なブロック（`[[` と `]]` の間）だけを取り出します。
* 生成された応答で入力ファイル（`ネクサス.txt`）を自動的に書き換えます。
* 各セッションの完全なコピーを `sesiones/` に、完全なログを `logs/` に保存します。

> 実行するたびにシステムは自己フィードバックします。**AI が自らを教える**のです。

## 📁 想定される構成

`ループAI.sh` スクリプトと同じディレクトリに以下のファイルが必要です。

* `llama-cli` → `llama.cpp` のバイナリ
* `mistral-7b-instruct-v0.1.Q6_K.gguf` → ローカルモデル
* `ループプロンプト.txt` → システムの初期プロンプト
* `ネクサス.txt` → 最初の入力、各応答で上書きされます

自動的に生成されるもの:

* `sesiones/` → 各セッションの .md コピー
* `logs/` → 実行の完全なログ

## 🧪 必要条件

* Bash（macOS / Linux）
* コンパイル済み `llama.cpp`
* 互換性のある `.gguf` モデル

## 🚀 実行方法

```bash
chmod +x ループAI.sh
./ループAI.sh
```

## 💡 創造的な利用

ループシミュレーション、自律的な物語エージェント、外部干渉なしで進化する必要のある AI の実験に最適です。
オフラインで再現可能な完全ローカル環境向けに設計されています。

---

**Eto Demerzel** によって作成されました

**Eto Demerzel** (Gustavo Silva Da Costa)
https://etodemerzel.gumroad.com  
https://github.com/BiblioGalactic
